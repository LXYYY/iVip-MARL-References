\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{liu_bucknall_2018}
\citation{liu_bucknall_2018}
\citation{LIU2015126}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Relationship between Formation Control and Cooperative Motion Planning }{1}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Formation Control}{1}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}System Architecture of Multi Agents Formation}{1}{subsection.3.1}\protected@file@percent }
\citation{campbell2012review}
\citation{liu_bucknall_2018}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Comparison of formation control and cooperative motion planning \cite  {liu_bucknall_2018}.}}{2}{figure.1}\protected@file@percent }
\newlabel{fig:CompFcCmp}{{1}{2}{Comparison of formation control and cooperative motion planning \cite {liu_bucknall_2018}}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}System Patterns of Formation}{2}{subsection.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces A formationc control architecture.}}{3}{figure.2}\protected@file@percent }
\newlabel{fig:FormationSystemArchitecture}{{2}{3}{A formationc control architecture}{figure.2}{}}
\citation{liu_bucknall_2018}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Patterns of formation.}}{4}{figure.3}\protected@file@percent }
\newlabel{fig:patternsformation}{{3}{4}{Patterns of formation}{figure.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Review of formation control strategies}{4}{subsection.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Three different types of formation shape maintenance. (a) Formation generation and maintenance. (b) Formation maintenance while tracking trajectory. (c) Formation shape variation and re-generation.}}{5}{figure.4}\protected@file@percent }
\newlabel{fig:3typesformationmaintenance}{{4}{5}{Three different types of formation shape maintenance. (a) Formation generation and maintenance. (b) Formation maintenance while tracking trajectory. (c) Formation shape variation and re-generation}{figure.4}{}}
\citation{knopp2017formation}
\citation{maei2010gq}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Comparison of formation control strategies.}}{6}{figure.5}\protected@file@percent }
\newlabel{fig:diffformationmethods}{{5}{6}{Comparison of formation control strategies}{figure.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Cooperative Formation Path Planning}{6}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Formation Control using Reinforcement Learning}{6}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Leader-Follower Fromation using GQ($\lambda $) Reinforcement Learning}{6}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.1}Framework}{6}{subsubsection.5.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Comparison of optimisation objectives of single and multi agent path planning.}}{7}{figure.6}\protected@file@percent }
\newlabel{fig:compsinglemultipathplanning}{{6}{7}{Comparison of optimisation objectives of single and multi agent path planning}{figure.6}{}}
\newlabel{sfig:leaderfollowerrlstate}{{7(a)}{8}{Subfigure 7(a)}{subfigure.7.1}{}}
\newlabel{sub@sfig:leaderfollowerrlstate}{{(a)}{8}{Subfigure 7(a)\relax }{subfigure.7.1}{}}
\newlabel{sfig:leaderfollowerrltilecoding}{{7(b)}{8}{Subfigure 7(b)}{subfigure.7.2}{}}
\newlabel{sub@sfig:leaderfollowerrltilecoding}{{(b)}{8}{Subfigure 7(b)\relax }{subfigure.7.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces States and tile coding of leader-follower using RL method.}}{8}{figure.7}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Definition of state space.}}}{8}{figure.7}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {9 $\times $ 9 tile coding of state.}}}{8}{figure.7}\protected@file@percent }
\newlabel{fig:leaderfollowerrl}{{7}{8}{States and tile coding of leader-follower using RL method}{figure.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.2}State Representation}{8}{subsubsection.5.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.3}Reinforcements}{8}{subsubsection.5.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Simple four agent line formation in V-REP,: the first leader can be found in the lower left corner. The red cones are showing active proximity sensors.}}{9}{figure.8}\protected@file@percent }
\newlabel{fig:leaderfollowerrlresult1}{{8}{9}{Simple four agent line formation in V-REP,: the first leader can be found in the lower left corner. The red cones are showing active proximity sensors}{figure.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.4}Experiment}{9}{subsubsection.5.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.5}Discussion}{9}{subsubsection.5.1.5}\protected@file@percent }
\citation{aykin2018deep}
\citation{johns2018intelligent}
\citation{balch1998behavior}
\citation{johns2018intelligent}
\citation{dhariwal2017openai}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Distribution of visited states at around 1.3 million steps. In 74\% of all cases, the leader was detected in the dark red area right to our optimal region. It stayed within the orange/red 3×3 area on the right side of the optimal region in 98.7\% of the time. To make the rarely visited states discernible, a logarithmic scale is used: red and orange values are in the range of 30\%–1\%, green values are at around 0.1\%, and blue values are another magnitude below that. The non-colored states were never visited in this experiment.}}{10}{figure.9}\protected@file@percent }
\newlabel{fig:leaderfollowerrlresult2}{{9}{10}{Distribution of visited states at around 1.3 million steps. In 74\% of all cases, the leader was detected in the dark red area right to our optimal region. It stayed within the orange/red 3×3 area on the right side of the optimal region in 98.7\% of the time. To make the rarely visited states discernible, a logarithmic scale is used: red and orange values are in the range of 30\%–1\%, green values are at around 0.1\%, and blue values are another magnitude below that. The non-colored states were never visited in this experiment}{figure.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}DRL-enhanced Behaviour-based Method}{10}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.1}Framework}{10}{subsubsection.5.2.1}\protected@file@percent }
\citation{brockman2016openai}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces The parameter shared DRL model used by all agents.}}{11}{figure.10}\protected@file@percent }
\newlabel{fig:drlbbmparamsharing}{{10}{11}{The parameter shared DRL model used by all agents}{figure.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.2}State Representation}{11}{subsubsection.5.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.3}Reinforcements}{11}{subsubsection.5.2.3}\protected@file@percent }
\newlabel{eq:drlbbreward}{{1}{11}{Reinforcements}{equation.5.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.4}Experiment}{11}{subsubsection.5.2.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces The relationship between the distance measured by a sensor and the sensor’s power in an agents state space.}}{12}{figure.11}\protected@file@percent }
\newlabel{fig:sensorpower}{{11}{12}{The relationship between the distance measured by a sensor and the sensor’s power in an agents state space}{figure.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces The experiment simulation environment of DRL-enhanced bahaviour-based method.}}{13}{figure.12}\protected@file@percent }
\newlabel{fig:drlbbexperiment}{{12}{13}{The experiment simulation environment of DRL-enhanced bahaviour-based method}{figure.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces A sample result of the experiments of DRL-enhanced Behaviour-based Method.}}{13}{figure.13}\protected@file@percent }
\newlabel{fig:drlbbexperimentresult}{{13}{13}{A sample result of the experiments of DRL-enhanced Behaviour-based Method}{figure.13}{}}
\citation{rawat2020multi}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces A sample learning cureve of the experiments of DRL-enhanced Behaviour-based Method.}}{14}{figure.14}\protected@file@percent }
\newlabel{fig:drlbblearningcurve}{{14}{14}{A sample learning cureve of the experiments of DRL-enhanced Behaviour-based Method}{figure.14}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.5}Discussion}{14}{subsubsection.5.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Coordinating Formation Control using Reinforcement Learning}{14}{subsection.5.3}\protected@file@percent }
\citation{foerster2018counterfactual,lowe2017multi}
\citation{foerster2018counterfactual}
\citation{andrychowicz2017hindsight}
\citation{hendrickx2005rigidity}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.1}Framework}{15}{subsubsection.5.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.2}State Representation}{15}{subsubsection.5.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.3}Reinforcement}{15}{subsubsection.5.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces The training involves sampling a batch of experiences, then modifying some of the experiences for positive reinforcement.}}{16}{figure.15}\protected@file@percent }
\newlabel{fig:mobilerobotrlcritic}{{15}{16}{The training involves sampling a batch of experiences, then modifying some of the experiences for positive reinforcement}{figure.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Interaction of agents with the environment. The state of the environment is observed through the observation function $Z_{i}$ of the agent $ i $. The agent acts according to its policy $ \pi _{i} $. The actions of all the agents $ \bar  {U} $ transitions the environment to some new state.}}{17}{figure.16}\protected@file@percent }
\newlabel{fig:multirobotfcframework}{{16}{17}{Interaction of agents with the environment. The state of the environment is observed through the observation function $Z_{i}$ of the agent $ i $. The agent acts according to its policy $ \pi _{i} $. The actions of all the agents $ \bar {U} $ transitions the environment to some new state}{figure.16}{}}
\citation{brockman2016openai}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces The plots on the left show the error in each edge of the formation. The plots on the right show the distance between the centroid of the formation and the goal. The edges between agents are denoted by e1, e2 and e3. The dotted red line denotes the mean error of all the edges.}}{18}{figure.17}\protected@file@percent }
\newlabel{fig:mobilerobotrlresult}{{17}{18}{The plots on the left show the error in each edge of the formation. The plots on the right show the distance between the centroid of the formation and the goal. The edges between agents are denoted by e1, e2 and e3. The dotted red line denotes the mean error of all the edges}{figure.17}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.4}Experiment}{18}{subsubsection.5.3.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Average reward per episode vs. number of training episodes.}}{19}{figure.18}\protected@file@percent }
\newlabel{fig:mobilerobotrllearningcurve}{{18}{19}{Average reward per episode vs. number of training episodes}{figure.18}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.5}Discussion}{19}{subsubsection.5.3.5}\protected@file@percent }
\bibstyle{ieeetr}
\bibdata{../iVip_ref}
\bibcite{liu_bucknall_2018}{1}
\bibcite{LIU2015126}{2}
\bibcite{campbell2012review}{3}
\bibcite{knopp2017formation}{4}
\bibcite{aykin2018deep}{5}
\bibcite{maei2010gq}{6}
\bibcite{johns2018intelligent}{7}
\bibcite{balch1998behavior}{8}
\bibcite{dhariwal2017openai}{9}
\bibcite{brockman2016openai}{10}
\bibcite{rawat2020multi}{11}
\bibcite{foerster2018counterfactual}{12}
\bibcite{lowe2017multi}{13}
\bibcite{andrychowicz2017hindsight}{14}
\bibcite{hendrickx2005rigidity}{15}
